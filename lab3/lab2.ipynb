{"cells":[{"cell_type":"code","source":["!pip install arxiv sentence-transformers faiss-cpu transformers torch rapidfuzz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lU7LhL0tMThc","executionInfo":{"status":"ok","timestamp":1762190758519,"user_tz":-180,"elapsed":6257,"user":{"displayName":"","userId":""}},"outputId":"b5fba9dd-ced7-493b-e927-ac1c43ae0130"},"id":"lU7LhL0tMThc","execution_count":218,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: arxiv in /usr/local/lib/python3.12/dist-packages (2.3.0)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n","Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (3.14.3)\n","Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.12/dist-packages (from arxiv) (6.0.12)\n","Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.12/dist-packages (from arxiv) (2.32.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.12/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2025.10.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"]}]},{"cell_type":"code","source":["import os\n","import re\n","import json\n","from typing import List, Dict, Tuple\n","\n","import arxiv\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","import faiss\n","from rapidfuzz import process, fuzz\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","import torch\n","import matplotlib.pyplot as plt\n","\n","\n","ARXIV_QUERY = \"cat:cs.LG\"\n","MAX_RESULTS = 300\n","EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'\n","FAISS_INDEX_PATH = 'faiss_index.bin'\n","METADATA_PATH = 'metadata.json'\n","LLM_NAME = 'google/flan-t5-small'"],"metadata":{"id":"QOoyU7hSLD6I","executionInfo":{"status":"ok","timestamp":1762190758528,"user_tz":-180,"elapsed":1,"user":{"displayName":"","userId":""}}},"id":"QOoyU7hSLD6I","execution_count":219,"outputs":[]},{"cell_type":"code","source":["def fetch_arxiv(query: str, max_results: int) -> List[Dict]:\n","    client = arxiv.Client()\n","    search = arxiv.Search(\n","        query=query,\n","        max_results=max_results,\n","        sort_by=arxiv.SortCriterion.SubmittedDate\n","    )\n","    results = []\n","\n","    try:\n","          for r in client.results(search):\n","            results.append({\n","                'title': r.title,\n","                'authors': [a.name for a in r.authors],\n","                'summary': r.summary\n","            })\n","    except arxiv.UnexpectedEmptyPageError as e:\n","          print(f\"Stopped fetching results: {e}\")\n","\n","    return results"],"metadata":{"id":"SeUbQo3IL1D7","executionInfo":{"status":"ok","timestamp":1762191099585,"user_tz":-180,"elapsed":42,"user":{"displayName":"","userId":""}}},"id":"SeUbQo3IL1D7","execution_count":244,"outputs":[]},{"cell_type":"code","source":["articles = fetch_arxiv(query=ARXIV_QUERY, max_results=MAX_RESULTS)"],"metadata":{"id":"sLTRVz7Vi7Zr","executionInfo":{"status":"ok","timestamp":1762191108083,"user_tz":-180,"elapsed":7406,"user":{"displayName":"","userId":""}}},"id":"sLTRVz7Vi7Zr","execution_count":245,"outputs":[]},{"cell_type":"code","source":["print(len(articles))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_5E3VeloPms","executionInfo":{"status":"ok","timestamp":1762191118948,"user_tz":-180,"elapsed":4,"user":{"displayName":"","userId":""}},"outputId":"c667b815-fe3a-46bc-9808-68f855b0557c"},"id":"X_5E3VeloPms","execution_count":246,"outputs":[{"output_type":"stream","name":"stdout","text":["300\n"]}]},{"cell_type":"code","source":["def articles_eda(articles: any):\n","  word_counts = [len(entry[\"summary\"].split()) for entry in articles]\n","\n","  plt.hist(word_counts, bins=10, edgecolor='black')\n","  plt.xlabel(\"Number of words\")\n","  plt.ylabel(\"Amount of articles\")\n","  plt.title(\"Distribution of articles by 'summary' word length\")\n","  plt.show()\n"],"metadata":{"id":"ZTEmraOQhlrd","executionInfo":{"status":"ok","timestamp":1762191123872,"user_tz":-180,"elapsed":3,"user":{"displayName":"","userId":""}}},"id":"ZTEmraOQhlrd","execution_count":247,"outputs":[]},{"cell_type":"code","source":["articles_eda(articles)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"z4BdJ1zUi-If","executionInfo":{"status":"ok","timestamp":1762191126033,"user_tz":-180,"elapsed":834,"user":{"displayName":"","userId":""}},"outputId":"faafa3fa-11a2-4f94-de42-fe75b1b5103f"},"id":"z4BdJ1zUi-If","execution_count":248,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["CHUNK_SIZE_WORDS = 175\n","CHUNK_OVERLAP_WORDS = 40"],"metadata":{"id":"JivkG1H4i-tA","executionInfo":{"status":"ok","timestamp":1762191136863,"user_tz":-180,"elapsed":40,"user":{"displayName":"","userId":""}}},"id":"JivkG1H4i-tA","execution_count":249,"outputs":[]},{"cell_type":"code","source":["def split_into_chunks(text: str, chunk_size_words=CHUNK_SIZE_WORDS, overlap_words=CHUNK_OVERLAP_WORDS) -> List[str]:\n","    words = re.findall(r\"\\b[\\w'-]+\\b\", text)\n","    chunks = []\n","    i = 0\n","    while i < len(words):\n","        chunk_words = words[i:i+chunk_size_words]\n","        chunks.append(' '.join(chunk_words))\n","        i += chunk_size_words - overlap_words\n","    return chunks"],"metadata":{"id":"3C0kCnWkL3uS","executionInfo":{"status":"ok","timestamp":1762191137589,"user_tz":-180,"elapsed":5,"user":{"displayName":"","userId":""}}},"id":"3C0kCnWkL3uS","execution_count":250,"outputs":[]},{"cell_type":"code","source":["def build_embeddings_and_faiss(corpus_chunks: List[str], model_name=EMBEDDING_MODEL_NAME) -> Tuple[faiss.IndexFlatIP, np.ndarray]:\n","    model = SentenceTransformer(model_name)\n","    embeddings = model.encode(corpus_chunks, show_progress_bar=True, convert_to_numpy=True)\n","    faiss.normalize_L2(embeddings)\n","    dim = embeddings.shape[1]\n","    index = faiss.IndexFlatIP(dim)\n","    index.add(embeddings)\n","    return index, embeddings"],"metadata":{"id":"4ePdIZybL4_K","executionInfo":{"status":"ok","timestamp":1762191138813,"user_tz":-180,"elapsed":4,"user":{"displayName":"","userId":""}}},"id":"4ePdIZybL4_K","execution_count":251,"outputs":[]},{"cell_type":"code","source":["def save_index(index: faiss.IndexFlatIP, path=FAISS_INDEX_PATH):\n","    faiss.write_index(index, path)"],"metadata":{"id":"x1XeMMmJL63O","executionInfo":{"status":"ok","timestamp":1762191139835,"user_tz":-180,"elapsed":4,"user":{"displayName":"","userId":""}}},"id":"x1XeMMmJL63O","execution_count":252,"outputs":[]},{"cell_type":"code","source":["def load_index(path=FAISS_INDEX_PATH):\n","    return faiss.read_index(path)"],"metadata":{"id":"4NZNfKVzL7tU","executionInfo":{"status":"ok","timestamp":1762191140786,"user_tz":-180,"elapsed":3,"user":{"displayName":"","userId":""}}},"id":"4NZNfKVzL7tU","execution_count":253,"outputs":[]},{"cell_type":"code","source":["def prepare_corpus_and_index(articles: List[Dict]):\n","    chunks = []\n","    chunk_meta = []\n","    for art in articles:\n","        cs = split_into_chunks(art['summary'])\n","        for idx, c in enumerate(cs):\n","            chunks.append(c)\n","            chunk_meta.append({'title': art['title'], 'authors': art['authors'], 'chunk_index': idx})\n","    index, embeddings = build_embeddings_and_faiss(chunks)\n","    with open(METADATA_PATH, 'w', encoding='utf-8') as f:\n","        json.dump({'chunks': chunks, 'chunk_meta': chunk_meta, 'articles': articles}, f, ensure_ascii=False, indent=2)\n","    save_index(index)\n","    return index, chunks, chunk_meta"],"metadata":{"id":"nQ8ugEcrL9ZV","executionInfo":{"status":"ok","timestamp":1762191141380,"user_tz":-180,"elapsed":10,"user":{"displayName":"","userId":""}}},"id":"nQ8ugEcrL9ZV","execution_count":254,"outputs":[]},{"cell_type":"code","source":["def create_vocabulary_from_corpus(chunks: List[str], top_k=2000) -> List[str]:\n","    freq = {}\n","    for c in chunks:\n","        for w in re.findall(r\"\\w+\", c.lower()):\n","            freq[w] = freq.get(w, 0) + 1\n","    vocab = [w for w, _ in sorted(freq.items(), key=lambda x: x[1], reverse=True)][:top_k]\n","    return vocab"],"metadata":{"id":"HOGdxtFTL-6r","executionInfo":{"status":"ok","timestamp":1762191142397,"user_tz":-180,"elapsed":3,"user":{"displayName":"","userId":""}}},"id":"HOGdxtFTL-6r","execution_count":255,"outputs":[]},{"cell_type":"code","source":["def correct_query_typos(query: str, vocab: List[str], scorer=fuzz.WRatio, limit=2) -> str:\n","    tokens = re.findall(r\"\\w+\", query)\n","    corrected_tokens = []\n","    for t in tokens:\n","        matches = process.extract(t.lower(), vocab, scorer=scorer, limit=1)\n","        if matches:\n","            best, score, _ = matches[0]\n","            if score >= 93 and best != t.lower():\n","                corrected_tokens.append(best)\n","            else:\n","                corrected_tokens.append(t)\n","        else:\n","            corrected_tokens.append(t)\n","    return ' '.join(corrected_tokens)"],"metadata":{"id":"gykUQIJVMBfP","executionInfo":{"status":"ok","timestamp":1762191143154,"user_tz":-180,"elapsed":4,"user":{"displayName":"","userId":""}}},"id":"gykUQIJVMBfP","execution_count":256,"outputs":[]},{"cell_type":"code","source":["def retrieve(query: str, index: faiss.IndexFlatIP, chunks: List[str], top_k=5, model_name=EMBEDDING_MODEL_NAME) -> List[Tuple[int, float]]:\n","    model = SentenceTransformer(model_name)\n","    q_emb = model.encode([query], convert_to_numpy=True)\n","    faiss.normalize_L2(q_emb)\n","    D, I = index.search(q_emb, top_k)\n","    return list(zip(I[0].tolist(), D[0].tolist()))"],"metadata":{"id":"SWOWUCnuMGY3","executionInfo":{"status":"ok","timestamp":1762191144613,"user_tz":-180,"elapsed":4,"user":{"displayName":"","userId":""}}},"id":"SWOWUCnuMGY3","execution_count":257,"outputs":[]},{"cell_type":"code","source":["class LLM:\n","    def __init__(self, model_name=LLM_NAME):\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","\n","    def generate(self, prompt: str, max_length=256) -> str:\n","        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n","        with torch.no_grad():\n","            outputs = self.model.generate(**inputs, max_length=max_length)\n","        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"],"metadata":{"id":"7gC7DvKfMHtl","executionInfo":{"status":"ok","timestamp":1762191145552,"user_tz":-180,"elapsed":5,"user":{"displayName":"","userId":""}}},"id":"7gC7DvKfMHtl","execution_count":258,"outputs":[]},{"cell_type":"code","source":["def build_rag_prompt(retrieved_chunks: List[str], user_query: str, instruction: str=\"You're assistant with specialization in machine learning, help with the following request using provied context. Response should be concise\") -> str:\n","    context = '\\n'.join(retrieved_chunks)\n","    prompt = f\"{instruction}: {user_query}\\n\\nContext:\\n{context}\"\n","    return prompt\n","\n","def answer_with_rag(user_query: str, index: faiss.IndexFlatIP, chunks: List[str], chunk_meta: List[Dict], vocab: List[str], llm: LLM, top_k=5) -> Dict:\n","    corrected_query = correct_query_typos(user_query, vocab)\n","    prompt = build_rag_prompt([], corrected_query)\n","    answer = llm.generate(corrected_query)\n","\n","    results = retrieve(corrected_query, index, chunks, top_k=top_k)\n","    retrieved_texts = [chunks[i] for i, _ in results]\n","    rag_prompt = build_rag_prompt(retrieved_texts, corrected_query)\n","    rag_answer = llm.generate(rag_prompt)\n","    return {\n","        'corrected_query': corrected_query,\n","        'retrieved_chunks_idx_and_scores': results,\n","        'prompt': prompt,\n","        'answer': answer,\n","        'rag_prompt': rag_prompt,\n","        'rag_answer': rag_answer\n","    }\n","\n"],"metadata":{"id":"ZujqCX7pIWmL","executionInfo":{"status":"ok","timestamp":1762191479540,"user_tz":-180,"elapsed":5,"user":{"displayName":"","userId":""}}},"id":"ZujqCX7pIWmL","execution_count":268,"outputs":[]},{"cell_type":"code","source":["index, chunks, chunk_meta = prepare_corpus_and_index(articles)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["f3beda65d2624ee3890ff2f9bdbdd2f3","60849580f76949d9802cad3edc9e0375","d4ea0293edac465eb149adfa2d85e462","380b140b5ff14acd81d375e431f0bfdc","7b91c1faf5fc411a8c8cb44576269789","d6129194a56b4022bfb994b02196f5fd","385c73ec1f084e4ebc0afd604eff98ee","9383b7eef4cb4897934c7ef416a9ac02","8f1cd4a06156448990aa3abfe56dbf95","6bdd160a6ef04ce1bda15ec1d95c9eb7","b85c46711a3749a1adf9f9da5b2b881b"]},"id":"UH_6-TVcNHbR","executionInfo":{"status":"ok","timestamp":1762191203484,"user_tz":-180,"elapsed":55101,"user":{"displayName":"","userId":""}},"outputId":"8b94ef81-09a5-4f60-aa81-fd9ceb660279"},"id":"UH_6-TVcNHbR","execution_count":260,"outputs":[{"output_type":"display_data","data":{"text/plain":["Batches:   0%|          | 0/18 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3beda65d2624ee3890ff2f9bdbdd2f3"}},"metadata":{}}]},{"cell_type":"code","source":["print(\"Chunk examples:\\n\",chunks[0],\"\\n\",chunks[int(len(chunks)/2)],\"\\n\",chunks[int(len(chunks)*3/4)])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AiS34pzRfWlM","executionInfo":{"status":"ok","timestamp":1762191203496,"user_tz":-180,"elapsed":11,"user":{"displayName":"","userId":""}},"outputId":"c41730aa-4d2d-4f39-f2ae-668ba0076767"},"id":"AiS34pzRfWlM","execution_count":261,"outputs":[{"output_type":"stream","name":"stdout","text":["Chunk examples:\n"," The efficiency of large language models LLMs is fundamentally limited by their sequential token-by-token generation process We argue that overcoming this bottleneck requires a new design axis for LLM scaling increasing the semantic bandwidth of each generative step To this end we introduce Continuous Autoregressive Language Models CALM a paradigm shift from discrete next-token prediction to continuous next-vector prediction CALM uses a high-fidelity autoencoder to compress a chunk of K tokens into a single continuous vector from which the original tokens can be reconstructed with over 99 9 accuracy This allows us to model language as a sequence of continuous vectors instead of discrete tokens which reduces the number of generative steps by a factor of K The paradigm shift necessitates a new modeling toolkit therefore we develop a comprehensive likelihood-free framework that enables robust training evaluation and controllable sampling in the continuous domain Experiments show that CALM significantly improves the performance-compute trade-off achieving the performance of strong discrete baselines at a significantly lower computational cost More importantly these findings establish next-vector prediction as a \n"," Bayesian Optimization BO has the potential to solve various combinatorial tasks ranging from materials science to neural architecture search However BO requires specialized kernels to effectively model combinatorial domains Recent efforts have introduced several combinatorial kernels but the relationships among them are not well understood To bridge this gap we develop a unifying framework based on heat kernels which we derive in a systematic way and express as simple closed-form expressions Using this framework we prove that many successful combinatorial kernels are either related or equivalent to heat kernels and validate this theoretical claim in our experiments Moreover our analysis confirms and extends the results presented in Bounce certain algorithms performance decreases substantially when the unknown optima of the function do not have a certain structure In contrast heat kernels are not sensitive to the location of the optima Lastly we show that a fast and simple pipeline relying on heat kernels is able to achieve state-of-the-art results matching or even outperforming certain slow or complex algorithms \n"," Traffic congestion in urban road networks leads to longer trip times and higher emissions especially during peak periods While the Shortest Path First SPF algorithm is optimal for a single vehicle in a static network it performs poorly in dynamic multi-vehicle settings often worsening congestion by routing all vehicles along identical paths We address dynamic vehicle routing through a multi-agent reinforcement learning MARL framework for coordinated network-aware fleet navigation We first propose Adaptive Navigation AN a decentralized MARL model where each intersection agent provides routing guidance based on i local traffic and ii neighborhood state modeled using Graph Attention Networks GAT To improve scalability in large networks we further propose Hierarchical Hub-based Adaptive Navigation HHAN an extension of AN that assigns agents only to key intersections hubs Vehicles are routed hub-to-hub under agent control while SPF handles micro-routing within each hub region For hub coordination HHAN adopts centralized training with decentralized execution CTDE under the Attentive Q-Mixing A-QMIX framework which aggregates asynchronous vehicle decisions via attention Hub agents use flow-aware state features that combine local\n"]}]},{"cell_type":"code","source":["vocab = create_vocabulary_from_corpus(chunks)"],"metadata":{"id":"gtpzJHqBNJMs","executionInfo":{"status":"ok","timestamp":1762191203529,"user_tz":-180,"elapsed":28,"user":{"displayName":"","userId":""}}},"id":"gtpzJHqBNJMs","execution_count":262,"outputs":[]},{"cell_type":"code","source":["llm = LLM()"],"metadata":{"id":"FkYaZkIENFFJ","executionInfo":{"status":"ok","timestamp":1762191203935,"user_tz":-180,"elapsed":401,"user":{"displayName":"","userId":""}}},"id":"FkYaZkIENFFJ","execution_count":263,"outputs":[]},{"cell_type":"code","source":["questions = [\n","    \"What is Combinatorial Fusion Analysis?\",\n","    \"What's the best algorithm?\",\n","    \"What are typical mistakes?\",\n","    \"How to improve model efficency?\",\n","    \"List model learning algorithms\"\n","]"],"metadata":{"id":"sHMcarDrPC58","executionInfo":{"status":"ok","timestamp":1762191203943,"user_tz":-180,"elapsed":4,"user":{"displayName":"","userId":""}}},"id":"sHMcarDrPC58","execution_count":264,"outputs":[]},{"cell_type":"code","source":["for question in questions:\n","    result = answer_with_rag(question, index, chunks, chunk_meta, vocab, llm, top_k=10)\n","    print(\"question:\", question)\n","    print(\"------------------------------------------------\")\n","    print(\"answer:\", result['answer'])\n","    print(\"rag_answer:\", result['rag_answer'])\n","    print(\"------------------------------------------------\")\n","    print(\"corrected_query:\", result['corrected_query'])\n","    print(\"rag_prompt:\", result['rag_prompt'])\n","    print(\"------------------------------------------------\")\n","    print(\"------------------------------------------------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q23vLCG5OY3s","executionInfo":{"status":"ok","timestamp":1762191967245,"user_tz":-180,"elapsed":25104,"user":{"displayName":"","userId":""}},"outputId":"43d49e17-349c-4bdf-ff31-afc7094c55bd"},"id":"q23vLCG5OY3s","execution_count":272,"outputs":[{"output_type":"stream","name":"stdout","text":["question: What is Combinatorial Fusion Analysis?\n","------------------------------------------------\n","answer: Combinatorial Fusion Analysis The combination of fusion and fusion analysis is a method of combining fusion and fusion.\n","rag_answer: a novel approach to sentiment classification using the application of Combinatorial Fusion Analysis CFA to integrate an ensemble of diverse machine learning models achieving state-of-the-art accuracy on the IMDB sentiment analysis dataset of 97 072 CFA leverages the concept of cognitive diversity which utilizes rank-score characteristic functions to quantify the dissimilarity between models and strategically combine their predictions\n","------------------------------------------------\n","corrected_query: What is Combinatorial Fusion Analysis\n","rag_prompt: You're assistant with specialization in machine learning, help with the following request using provied context. Response should be concise: What is Combinatorial Fusion Analysis\n","\n","Context:\n","This paper presents a novel approach to sentiment classification using the application of Combinatorial Fusion Analysis CFA to integrate an ensemble of diverse machine learning models achieving state-of-the-art accuracy on the IMDB sentiment analysis dataset of 97 072 CFA leverages the concept of cognitive diversity which utilizes rank-score characteristic functions to quantify the dissimilarity between models and strategically combine their predictions This is in contrast to the common process of scaling the size of individual models and thus is comparatively efficient in computing resource use Experimental results also indicate that CFA outperforms traditional ensemble methods by effectively computing and employing model diversity The approach in this paper implements the combination of a transformer-based model of the RoBERTa architecture with traditional machine learning models including Random Forest SVM and XGBoost\n","produce predictions In addition to achieving superior performance over the state-of-the-art models on benchmark datasets including CMU-MOSI CMU-MOSEI and CH-SIMS QiNN-QJ facilitates enhanced post-hoc interpretability through von-Neumann entanglement entropy This work establishes a principled framework for entangled multimodal fusion and paves the way for quantum-inspired approaches in modelling complex cross-modal correlations\n","Bayesian Optimization BO has the potential to solve various combinatorial tasks ranging from materials science to neural architecture search However BO requires specialized kernels to effectively model combinatorial domains Recent efforts have introduced several combinatorial kernels but the relationships among them are not well understood To bridge this gap we develop a unifying framework based on heat kernels which we derive in a systematic way and express as simple closed-form expressions Using this framework we prove that many successful combinatorial kernels are either related or equivalent to heat kernels and validate this theoretical claim in our experiments Moreover our analysis confirms and extends the results presented in Bounce certain algorithms performance decreases substantially when the unknown optima of the function do not have a certain structure In contrast heat kernels are not sensitive to the location of the optima Lastly we show that a fast and simple pipeline relying on heat kernels is able to achieve state-of-the-art results matching or even outperforming certain slow or complex algorithms\n","by 32 22 compared to baseline methods resulting in cleaner and more coherent graph structures These improvements establish LINK-KG as a strong foundation for analyzing complex criminal networks\n","phenomena under a single objective our framework clarifies the computational structure of learning and offers a principled foundation for designing adaptive algorithms\n","multi-cycle process and cross-validating it among independent author teams and peers the study provides a transparent and reproducible foundation for analyzing how GenAI affects SE processes methods and tools and for framing future research within this rapidly evolving area Based on these findings the article finally makes ten predictions for SE in the year 2030\n","Personalized treatment outcome prediction based on trial data for small-sample and rare patient groups is critical in precision medicine However the costly trial data limit the prediction performance To address this issue we propose a cross-fidelity knowledge distillation and adaptive fusion network CFKD-AFN which leverages abundant but low-fidelity simulation data to enhance predictions on scarce but high-fidelity trial data CFKD-AFN incorporates a dual-channel knowledge distillation module to extract complementary knowledge from the low-fidelity model along with an attention-guided fusion module to dynamically integrate multi-source information Experiments on treatment outcome prediction for the chronic obstructive pulmonary disease demonstrates significant improvements of CFKD-AFN over state-of-the-art methods in prediction accuracy ranging from 6 67 to 74 55 and strong robustness to varying high-fidelity dataset sizes Furthermore we extend CFKD-AFN to an interpretable variant enabling the exploration of latent medical semantics to support clinical decision-making\n","traditional methods and existing machine-learning techniques while reducing the reliance on multiple sensing modalities and eliminating the need for post-processing steps\n","an interesting log-likelihood interpretation where it is the optimal solution to the distributional multi-objective optimization problem We implemented IMG for a multi-objective molecule generation task Experiments show that IMG requiring only a single generation pass achieves a significantly higher hypervolume than baseline optimization algorithms that often require hundreds of diffusion generations Notably our algorithm can be viewed as an optimized diffusion process and can be integrated into existing methods to further improve their performance\n","We study the problem of nonparametric two-sample testing using the sliced Wasserstein SW distance While prior theoretical and empirical work indicates that the SW distance offers a promising balance between strong statistical guarantees and computational efficiency its theoretical foundations for hypothesis testing remain limited We address this gap by proposing a permutation-based SW test and analyzing its performance The test inherits finite-sample Type I error control from the permutation principle Moreover we establish non-asymptotic power bounds and show that the procedure achieves the minimax separation rate n 1 2 over multinomial and bounded-support alternatives matching the optimal guarantees of kernel-based tests while building on the geometric foundations of Wasserstein distances Our analysis further quantifies the trade-off between the number of projections and statistical power Finally numerical experiments demonstrate that the test combines finite-sample validity with competitive power and scalability and unlike kernel-based tests which require careful kernel tuning it performs consistently well across all scenarios we consider\n","------------------------------------------------\n","------------------------------------------------\n","question: What's the best algorithm?\n","------------------------------------------------\n","answer: a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a s a \n","rag_answer: CS-GRU\n","------------------------------------------------\n","corrected_query: What s the best algorithm\n","rag_prompt: You're assistant with specialization in machine learning, help with the following request using provied context. Response should be concise: What s the best algorithm\n","\n","Context:\n","location of the optima Lastly we show that a fast and simple pipeline relying on heat kernels is able to achieve state-of-the-art results matching or even outperforming certain slow or complex algorithms\n","outperforms state-of-the-art methods in accuracy with high robustness to domain shift and computational efficiency\n","7 offering an efficient and scalable solution that combines the robustness of Rand K with the strong performance of Top K\n","achieve excellent selection and detection accuracy with a low computational cost\n","phenomena under a single objective our framework clarifies the computational structure of learning and offers a principled foundation for designing adaptive algorithms\n","offer advantages in settings with high uncertainty where the difficulty arises from subtle differences between arm rewards In contrast classical algorithms often perform equally well or better in more separable scenarios or if fine-tuned extensively Our contributions are twofold 1 a framework for systematic evaluation of MAB algorithms and 2 insights into the conditions under which variance-aware approaches outperform their classical counterparts\n","models discovered by ELM significantly outperform existing methods\n","significantly lower bits than other quantization methods but achieves comparable or even higher performance\n","techniques\n","90 accuracy on sequential tasks and up to 99 31 on MNIST It is worth noting that our solution achieves 69 higher efficiency compared to SpikGRU The code is available at https github com YesmineAbdennadher CS-GRU\n","------------------------------------------------\n","------------------------------------------------\n","question: What are typical mistakes?\n","------------------------------------------------\n","answer: if you are a student, you should be able to use a calculator to calculate the cost of the project.\n","rag_answer: MisSynth is a pipeline that applies retrieval-augmented generation RAG to produce synthetic fallacy samples which are then used to fine-tune an LLM model\n","------------------------------------------------\n","corrected_query: What are typical mistakes\n","rag_prompt: You're assistant with specialization in machine learning, help with the following request using provied context. Response should be concise: What are typical mistakes\n","\n","Context:\n","While generative models especially large language models LLMs are ubiquitous in today's world principled mechanisms to assess their in correctness are limited Using the conformal prediction framework previous works construct sets of LLM responses where the probability of including an incorrect response or error is capped at a desired user-defined tolerance level However since these methods are based on p-values they are susceptible to p-hacking i e choosing the tolerance level post-hoc can invalidate the guarantees We therefore leverage e-values to complement generative model outputs with e-scores as a measure of incorrectness In addition to achieving the same statistical guarantees as before e-scores provide users flexibility in adaptively choosing tolerance levels after observing the e-scores themselves by upper bounding a post-hoc notion of error called size distortion We experimentally demonstrate their efficacy in assessing LLM outputs for different correctness types mathematical factuality and property constraints satisfaction\n","outputs for different correctness types mathematical factuality and property constraints satisfaction\n","Health-related misinformation is very prevalent and potentially harmful It is difficult to identify especially when claims distort or misinterpret scientific findings We investigate the impact of synthetic data generation and lightweight fine-tuning techniques on the ability of large language models LLMs to recognize fallacious arguments using the MISSCI dataset and framework In this work we propose MisSynth a pipeline that applies retrieval-augmented generation RAG to produce synthetic fallacy samples which are then used to fine-tune an LLM model Our results show substantial accuracy gains with fine-tuned models compared to vanilla baselines For instance the LLaMA 3 1 8B fine-tuned model achieved an over 35 F1-score absolute improvement on the MISSCI test split over its vanilla baseline We demonstrate that introducing synthetic fallacy data to augment limited annotated resources can significantly enhance zero-shot LLM classification performance on real-world scientific misinformation tasks even with limited computational resources The code and synthetic dataset are available on https github com mxpoliakov MisSynth\n","algorithms and frameworks We hope these findings motivate a broader reconsideration of precision trade-offs in RL fine-tuning\n","Large language models LLMs excel at numerical estimation but struggle to correctly quantify uncertainty We study how well LLMs construct confidence intervals around their own answers and find that they are systematically overconfident To evaluate this behavior we introduce FermiEval a benchmark of Fermi-style estimation questions with a rigorous scoring rule for confidence interval coverage and sharpness Across several modern models nominal 99 intervals cover the true answer only 65 of the time on average With a conformal prediction based approach that adjusts the intervals we obtain accurate 99 observed coverage and the Winkler interval score decreases by 54 We also propose direct log-probability elicitation and quantile adjustment methods which further reduce overconfidence at high confidence levels Finally we develop a perception-tunnel theory explaining why LLMs exhibit overconfidence when reasoning under uncertainty they act as if sampling from a truncated region of their inferred distribution neglecting its tails\n","Regression tasks notably in safety-critical domains require proper uncertainty quantification yet the literature remains largely classification-focused In this light we introduce a family of measures for total aleatoric and epistemic uncertainty based on proper scoring rules with a particular emphasis on kernel scores The framework unifies several well-known measures and provides a principled recipe for designing new ones whose behavior such as tail sensitivity robustness and out-of-distribution responsiveness is governed by the choice of kernel We prove explicit correspondences between kernel-score characteristics and downstream behavior yielding concrete design guidelines for task-specific measures Extensive experiments demonstrate that these measures are effective in downstream tasks and reveal clear trade-offs among instantiations including robustness and out-of-distribution detection performance\n","cues Because consistency training uses responses from the model itself as training data it avoids issues that arise from stale training data such as degrading model capabilities or enforcing outdated response guidelines While BCT and ACT reduce sycophancy equally well BCT does better at jailbreak reduction We think that BCT can simplify training pipelines by removing reliance on static datasets We argue that some alignment problems are better viewed not in terms of optimal responses but rather as consistency issues\n","minimal domain-consistent distractions and the proofs they generate frequently exhibit detours through irrelevant inferences\n","on real-world scientific misinformation tasks even with limited computational resources The code and synthetic dataset are available on https github com mxpoliakov MisSynth\n","Chain-of-thought CoT outputs let us read a model's step-by-step reasoning Since any long serial reasoning process must pass through this textual trace the quality of the CoT is a direct window into what the model is thinking This visibility could help us spot unsafe or misaligned behavior monitorability but only if the CoT is transparent about its internal reasoning faithfulness Fully measuring faithfulness is difficult so researchers often focus on examining the CoT in cases where the model changes its answer after adding a cue to the input This proxy finds some instances of unfaithfulness but loses information when the model maintains its answer and does not investigate aspects of reasoning not tied to the cue We extend these results to a more holistic sense of monitorability by introducing verbosity whether the CoT lists every factor needed to solve the task We combine faithfulness and verbosity into a single monitorability score that shows how well the CoT serves as the model's external working memory a property that many safety schemes based on CoT monitoring depend\n","------------------------------------------------\n","------------------------------------------------\n","question: How to improve model efficency?\n","------------------------------------------------\n","answer: How to improve model efficiency\n","rag_answer: We propose a method for reducing the cost-accuracy trade-off by minimizing the time and effort required to achieve the desired outcomes.\n","------------------------------------------------\n","corrected_query: How to improve model efficiency\n","rag_prompt: You're assistant with specialization in machine learning, help with the following request using provied context. Response should be concise: How to improve model efficiency\n","\n","Context:\n","2 times offering a genuinely memory-efficient and practical solution for fine-tuning large-scale pre-trained models\n","approach successfully routes over 80 of queries to the small model while incurring less than 10 drop in problem solving probability\n","How can we explain the influence of training data on black-box models Influence functions IFs offer a post-hoc solution by utilizing gradients and Hessians However computing the Hessian for an entire dataset is resource-intensive necessitating a feasible alternative A common approach involves randomly sampling a small subset of the training data but this method often results in highly inconsistent IF estimates due to the high variance in sample configurations To address this we propose two advanced sampling techniques based on features and logits These samplers select a small yet representative subset of the entire dataset by considering the stochastic distribution of features or logits thereby enhancing the accuracy of IF estimations We validate our approach through class removal experiments a typical application of IFs using the F1-score to measure how effectively the model forgets the removed class while maintaining inference consistency on the remaining classes Our method reduces computation time by 30 1 and memory usage by 42 2 or improves the F1-score by 2 5 compared to the baseline\n","models discovered by ELM significantly outperform existing methods\n","removed class while maintaining inference consistency on the remaining classes Our method reduces computation time by 30 1 and memory usage by 42 2 or improves the F1-score by 2 5 compared to the baseline\n","dataset and phase-specific tuning while producing better cost-accuracy tradeoff curves compared to standard methods Users can dynamically adjust the cost-accuracy trade-off through a continuous effort parameter specified at inference time We observe that the model automatically learns to allocate resources proportionally to the task difficulty and across model scales ranging from 1 5B to 32B parameters our approach enables approximately 3x reduction in chain-of-thought length while maintaining or improving performance relative to the base model used for RL training\n","Reasoning Fine-Tuning Model Alignment Cost-Efficient AI\n","Knowledge distillation KD is an effective method for model compression and transferring knowledge between models However its effect on model's robustness against spurious correlations that degrade performance on out-of-distribution data remains underexplored This study investigates the effect of knowledge distillation on the transferability of debiasing capabilities from teacher models to student models on natural language inference NLI and image classification tasks Through extensive experiments we illustrate several key findings i overall the debiasing capability of a model is undermined post-KD ii training a debiased model does not benefit from injecting teacher knowledge iii although the overall robustness of a model may remain stable post-distillation significant variations can occur across different types of biases and iv we pin-point the internal attention pattern and circuit that causes the distinct behavior post-KD Given the above findings we propose three effective solutions to improve the distillability of debiasing methods developing high quality data for augmentation implementing iterative knowledge distillation and initializing student models with weights obtained from teacher models To the best of our knowledge this is the first\n","Dataset distillation condenses large datasets into synthetic subsets achieving performance comparable to training on the full dataset while substantially reducing storage and computation costs Most existing dataset distillation methods assume that all real instances contribute equally to the process In practice real-world datasets contain both informative and redundant or even harmful instances and directly distilling the full dataset without considering data quality can degrade model performance In this work we present Influence-Weighted Distillation IWD a principled framework that leverages influence functions to explicitly account for data quality in the distillation process IWD assigns adaptive weights to each instance based on its estimated impact on the distillation objective prioritizing beneficial data while downweighting less useful or harmful ones Owing to its modular design IWD can be seamlessly integrated into diverse dataset distillation frameworks Our empirical results suggest that integrating IWD tends to improve the quality of distilled datasets and enhance model performance with accuracy gains of up to 7 8\n","Through both theoretical and empirical studies we validate the efficacy of our approach in achieving significant inversion acceleration up to 3 79 faster while maintaining comparable or even enhanced downstream performance in data-free model quantization and data-free knowledge transfer Code is available at https github com Egg-Hu SMI\n","------------------------------------------------\n","------------------------------------------------\n","question: List model learning algorithms\n","------------------------------------------------\n","answer: List model learning algorithms\n","rag_answer: and offers a principled foundation for designing adaptive algorithms models discovered by ELM significantly outperform existing methods Newton step with careful initializations\n","------------------------------------------------\n","corrected_query: List model learning algorithms\n","rag_prompt: You're assistant with specialization in machine learning, help with the following request using provied context. Response should be concise: List model learning algorithms\n","\n","Context:\n","phenomena under a single objective our framework clarifies the computational structure of learning and offers a principled foundation for designing adaptive algorithms\n","models discovered by ELM significantly outperform existing methods\n","Newton step with careful initializations We also give novel and non-trivial analyses including an induction method for analyzing the ell_1 norm error careful analyses on the covariance of non-independent random variables and a decomposition on the regret We further extend our algorithm to OSLR with additional observations where the algorithms can observe additional k_0 attributes after each prediction and improve previous regret bounds Kale et al 2017 Ito et al 2017\n","Recent research on time series foundation models has primarily focused on forecasting leaving it unclear how generalizable their learned representations are In this study we examine whether frozen pre-trained forecasting models can provide effective representations for classification To this end we compare different representation extraction strategies and introduce two model-agnostic embedding augmentations Our experiments show that the best forecasting models achieve classification accuracy that matches or even surpasses that of state-of-the-art models pre-trained specifically for classification Moreover we observe a positive correlation between forecasting and classification performance These findings challenge the assumption that task-specific pre-training is necessary and suggest that learning to forecast may provide a powerful route toward constructing general-purpose time series foundation models\n","Learning rules prescriptions for updating model parameters to improve performance are typically assumed rather than derived Why do some learning rules work better than others and under what assumptions can a given rule be considered optimal We propose a theoretical framework that casts learning rules as policies for navigating partially observable loss landscapes and identifies optimal rules as solutions to an associated optimal control problem A range of well-known rules emerge naturally within this framework under different assumptions gradient descent from short-horizon optimization momentum from longer-horizon planning natural gradients from accounting for parameter space geometry non-gradient rules from partial controllability and adaptive optimizers like Adam from online Bayesian inference of loss landscape shape We further show that continual learning strategies like weight resetting can be understood as optimal responses to task uncertainty By unifying these phenomena under a single objective our framework clarifies the computational structure of learning and offers a principled foundation for designing adaptive algorithms\n","In this paper we study the problem of online sparse linear regression OSLR where the algorithms are restricted to accessing only k out of d attributes per instance for prediction which was proved to be NP-hard Previous work gave polynomial-time algorithms assuming the data matrix satisfies the linear independence of features the compatibility condition or the restricted isometry property We introduce a new polynomial-time algorithm which significantly improves previous regret bounds Ito et al 2017 under the compatibility condition that is weaker than the other two assumptions The improvements benefit from a tighter convergence rate of the ell_1 norm error of our estimators Our algorithm leverages the well-studied Dantzig Selector but importantly with several novel techniques including an algorithm-dependent sampling scheme for estimating the covariance matrix an adaptive parameter tuning scheme and a batching online Newton step with careful initializations We also give novel and non-trivial analyses including an induction method for analyzing the ell_1 norm error careful analyses on the covariance of non-independent random variables and a decomposition on the regret We further extend\n","to query in order to balance cost and predictive performance We establish theoretical guarantees for both of our algorithms including generalization bounds and label complexity analyses Empirical results across several domains show that our algorithms substantially reduce training costs without sacrificing prediction accuracy demonstrating the practical value of our budget-aware deferral algorithms\n","How can we explain the influence of training data on black-box models Influence functions IFs offer a post-hoc solution by utilizing gradients and Hessians However computing the Hessian for an entire dataset is resource-intensive necessitating a feasible alternative A common approach involves randomly sampling a small subset of the training data but this method often results in highly inconsistent IF estimates due to the high variance in sample configurations To address this we propose two advanced sampling techniques based on features and logits These samplers select a small yet representative subset of the entire dataset by considering the stochastic distribution of features or logits thereby enhancing the accuracy of IF estimations We validate our approach through class removal experiments a typical application of IFs using the F1-score to measure how effectively the model forgets the removed class while maintaining inference consistency on the remaining classes Our method reduces computation time by 30 1 and memory usage by 42 2 or improves the F1-score by 2 5 compared to the baseline\n","This paper proposes a novel paradigm for machine learning that moves beyond traditional parameter optimization Unlike conventional approaches that search for optimal parameters within a fixed geometric space our core idea is to treat the model itself as a malleable geometric entity Specifically we optimize the metric tensor field on a manifold with a predefined topology thereby dynamically shaping the geometric structure of the model space To achieve this we construct a variational framework whose loss function carefully balances data fidelity against the intrinsic geometric complexity of the manifold The former ensures the model effectively explains observed data while the latter acts as a regularizer penalizing overly curved or irregular geometries to encourage simpler models and prevent overfitting To address the computational challenges of this infinite-dimensional optimization problem we introduce a practical method based on discrete differential geometry the continuous manifold is discretized into a triangular mesh and the metric tensor is parameterized by edge lengths enabling efficient optimization using automatic differentiation tools Theoretical analysis reveals a profound analogy between our framework and the Einstein-Hilbert\n","Data for training structural health monitoring SHM systems are often expensive and or impractical to obtain particularly for labelled data Population-based SHM PBSHM aims to address this limitation by leveraging data from multiple structures However data from different structures will follow distinct distributions potentially leading to large generalisation errors for models learnt via conventional machine learning methods To address this issue transfer learning in the form of domain adaptation DA can be used to align the data distributions Most previous approaches have only considered emph unsupervised DA where no labelled target data are available they do not consider how to incorporate these technologies in an online framework updating as labels are obtained throughout the monitoring campaign This paper proposes a Bayesian framework for DA in PBSHM that can improve unsupervised DA mappings using a limited quantity of labelled target data In addition this model is integrated into an active sampling strategy to guide inspections to select the most informative observations to label leading to further reductions in the required labelled data to learn a target\n","------------------------------------------------\n","------------------------------------------------\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[{"file_id":"https://github.com/DJustProgrammeR/DataAnalasys/blob/main/lab2/lab2.ipynb","timestamp":1762192404159}]},"widgets":{"application/vnd.jupyter.widget-state+json":{"f3beda65d2624ee3890ff2f9bdbdd2f3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_60849580f76949d9802cad3edc9e0375","IPY_MODEL_d4ea0293edac465eb149adfa2d85e462","IPY_MODEL_380b140b5ff14acd81d375e431f0bfdc"],"layout":"IPY_MODEL_7b91c1faf5fc411a8c8cb44576269789"}},"60849580f76949d9802cad3edc9e0375":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6129194a56b4022bfb994b02196f5fd","placeholder":"","style":"IPY_MODEL_385c73ec1f084e4ebc0afd604eff98ee","value":"Batches:100%"}},"d4ea0293edac465eb149adfa2d85e462":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9383b7eef4cb4897934c7ef416a9ac02","max":18,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8f1cd4a06156448990aa3abfe56dbf95","value":18}},"380b140b5ff14acd81d375e431f0bfdc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bdd160a6ef04ce1bda15ec1d95c9eb7","placeholder":"","style":"IPY_MODEL_b85c46711a3749a1adf9f9da5b2b881b","value":"18/18[00:54&lt;00:00,1.21s/it]"}},"7b91c1faf5fc411a8c8cb44576269789":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6129194a56b4022bfb994b02196f5fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"385c73ec1f084e4ebc0afd604eff98ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9383b7eef4cb4897934c7ef416a9ac02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f1cd4a06156448990aa3abfe56dbf95":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6bdd160a6ef04ce1bda15ec1d95c9eb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b85c46711a3749a1adf9f9da5b2b881b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}